{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 3E12-C811\n",
      "\n",
      " Directory of c:\\Users\\RAJAB IMAM\\Documents\\Python Projects\\Data Pipeline\n",
      "\n",
      "05/24/2022  09:39 AM    <DIR>          .\n",
      "05/24/2022  09:39 AM    <DIR>          ..\n",
      "05/23/2022  12:32 PM             3,458 csv_datasource.py\n",
      "11/29/2020  09:04 PM               425 Customer Contracts$.csv\n",
      "11/29/2020  09:04 PM               275 Customer Demo.csv\n",
      "11/29/2020  09:04 PM               247 Customer Engagements.csv\n",
      "05/23/2022  01:27 PM               436 customer_contracts.csv\n",
      "05/22/2022  07:52 PM               154 Pipfile\n",
      "05/22/2022  07:53 PM            24,563 Pipfile.lock\n",
      "01/08/2022  02:05 AM             1,326 USArrests.csv\n",
      "               8 File(s)         30,884 bytes\n",
      "               2 Dir(s)  99,587,137,536 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find csv files from working directory, isolate only csv, \n",
    "\n",
    "\n",
    "    #get names of only csv files\n",
    "csv_files = []\n",
    "for file in os.listdir(os.getcwd()):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_files.append(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv 'USArrests.csv' raw_data\n"
     ]
    }
   ],
   "source": [
    "#create a new directory\n",
    "dataset_dir = 'raw_data'\n",
    "  \n",
    "    #make dataset folder to process csv files\n",
    "try: \n",
    "      mkdir = 'mkdir {0}'.format(dataset_dir)\n",
    "      os.system(mkdir)\n",
    "except:\n",
    "      pass\n",
    "\n",
    "    #move csv files to dataset folder\n",
    "for csv in csv_files:\n",
    "      mv_file = \"mv '{0}' {1}\".format(csv, dataset_dir)\n",
    "      os.system(mv_file)\n",
    "print(mv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv 'Customer Contracts$.csv' raw_data\n",
      "mv 'Customer Demo.csv' raw_data\n",
      "mv 'Customer Engagements.csv' raw_data\n",
      "mv 'customer_contracts.csv' raw_data\n",
      "mv 'USArrests.csv' raw_data\n"
     ]
    }
   ],
   "source": [
    "for csv in csv_files:\n",
    "      mv_file = \"mv '{0}' {1}\".format(csv, dataset_dir)\n",
    "      os.system(mv_file)\n",
    "      print(mv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(dataset_dir, csv_files):\n",
    "  \n",
    "    data_path = os.getcwd()+'/'+dataset_dir+'/'\n",
    "\n",
    "    #loop through the files and create the dataframe\n",
    "    df = {}\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df[file] = pd.read_csv(data_path+file)\n",
    "        except UnicodeDecodeError:\n",
    "            df[file] = pd.read_csv(data_path+file, encoding=\"ISO-8859-1\") #if utf-8 encoding error\n",
    "        print(file)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tbl_name(filename):\n",
    "  \n",
    "    #rename csv, force lower case, no spaces, no dashes\n",
    "    clean_tbl_name = filename.lower().replace(\" \", \"_\").replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"$\",\"\").replace(\"%\",\"\")\n",
    "    \n",
    "    tbl_name = '{0}'.format(clean_tbl_name.split('.')[0])\n",
    "\n",
    "    return tbl_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_colname(dataframe):\n",
    "  \n",
    "    #force column names to be lower case, no spaces, no dashes\n",
    "    dataframe.columns = [x.lower().replace(\" \", \"_\").replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\").replace(\"%\",\"\") for x in dataframe.columns]\n",
    "    \n",
    "    #processing data\n",
    "    replacements = {\n",
    "        'timedelta64[ns]': 'varchar',\n",
    "        'object': 'varchar',\n",
    "        'float64': 'float',\n",
    "        'int64': 'int',\n",
    "        'datetime64': 'timestamp'\n",
    "    }\n",
    "\n",
    "    col_str = \", \".join(\"{} {}\".format(n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements)))\n",
    "    \n",
    "    return col_str, dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_db(host, dbname, user, password, tbl_name, col_str, file, dataframe, dataframe_columns):\n",
    "    host='localhost'\n",
    "    dbname='bigdata'\n",
    "    user='postgres'\n",
    "    password='Reason@2010'\n",
    "    conn_string = \"host=%s dbname=%s user=%s password=%s\" % (host, dbname, user, password)\n",
    "    #conn_string = \"host='localhost' dbname='bigdata' user='postgres' password='Reason@2010'\"\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "    print('opened database successfully')\n",
    "    \n",
    "    #drop table with same name\n",
    "    cursor.execute(\"drop table if exists %s;\" % (tbl_name))\n",
    "\n",
    "    #create table\n",
    "    cursor.execute(\"create table %s (%s);\" % (tbl_name, col_str))\n",
    "    print('{0} was created successfully'.format(tbl_name)) \n",
    "    \n",
    "    #insert values to table\n",
    "\n",
    "    #save df to csv\n",
    "    dataframe.to_csv(file, header=dataframe_columns, index=False, encoding='utf-8')\n",
    "\n",
    "    #open the csv file, save it as an object\n",
    "    my_file = open(file)\n",
    "    print('file opened in memory')\n",
    "    \n",
    "    #upload to db\n",
    "    SQL_STATEMENT = \"\"\"\n",
    "    COPY %s FROM STDIN WITH\n",
    "        CSV\n",
    "        HEADER\n",
    "        DELIMITER AS ','\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.copy_expert(sql=SQL_STATEMENT % tbl_name, file=my_file)\n",
    "    print('file copied to db')\n",
    "    \n",
    "    cursor.execute(\"grant select on table %s to public\" % tbl_name)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print('table {0} imported to db completed'.format(tbl_name))\n",
    "\n",
    "    return upload_to_db\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef9f62e36719628ef5b7636ed927846694f3d690e23fa17eff1776ebea81e684"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
